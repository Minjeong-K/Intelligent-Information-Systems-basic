{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 분류하기\n",
    "\n",
    "## 1. 분류기 이용하기\n",
    "\n",
    "분류를 하고 테스트 진행\n",
    "\n",
    "### 나이브 베이즈 분류기\n",
    "\n",
    "베이즈 정리에 기반한 통계적 분류 기법\n",
    "- 베이즈 정리?\n",
    "    - p(a|b) = p(a,b) / p(b) = p(b|a)p(a) / p(b)\n",
    "\n",
    "## 2. 텍스트 분류하기\n",
    "\n",
    "- 어떤 텍스트가 긍정문일 확률은?\n",
    "    - p(긍정|입력텍스트) = p(입력텍스트|긍정)p(긍정) / p(입력텍스트)\n",
    "    - p(부정|입력텍스트) = p(입력텍스트|부정)p(부정) / p(입력텍스트)\n",
    "----------------------\n",
    "\n",
    "**나이브** : 개별 단어끼리는 서로 조건부 독립이 성립한다는 순진한 믿음!\n",
    "\n",
    "**베이즈** : 베이즈 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영어로 먼저 연습해보기\n",
    "- 분류기? **NLTK** 이용\n",
    "\n",
    "- NLTK : 교육용으로 개발된 자연어 처리 및 텍스트 분석을 위한 패키지\n",
    "    - word_tokenize 와 NaiveBayse 이용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 불러오기\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\강민정\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I like you', 'pos'),\n",
       " ('I hate you', 'neg'),\n",
       " ('I enjoyed it', 'pos'),\n",
       " ('I hate it', 'neg')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 입력해주기\n",
    "# 튜플 형태로 입력\n",
    "\n",
    "train = [('I like you', 'pos'), ('I hate you', 'neg'), \n",
    "         ('I enjoyed it', 'pos'), ('I hate it', 'neg')]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK 의 Nat-iveBayesClassifier\n",
    "\n",
    "- p(pos|\"I like it\") = p(\"I\"|pos)p(\"like\"|pos)p(\"it\"|pos)\n",
    "- p(neg|\"I hate it\") = p(\"I\"|neg)p(\"hate\"|neg)p(\"it\"|neg)\n",
    "\n",
    "## Train 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I', 'enjoyed', 'hate', 'it', 'like', 'you'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. train에 있는 문장들의 단어 set 만들기\n",
    "# set()을 이용하여 집합으로 만들기\n",
    "\n",
    "all_words = set()  # 빈 집합 선언\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]   # tup[0] : 문장, tup[1] : pos/neg\n",
    "    words = word_tokenize(sent)\n",
    "    for word in words:\n",
    "        all_words.add(word)\n",
    "\n",
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'enjoyed': False, 'you': True, 'it': False, 'like': True, 'I': True, 'hate': False}, 'pos')\n",
      "({'enjoyed': False, 'you': True, 'it': False, 'like': False, 'I': True, 'hate': True}, 'neg')\n",
      "({'enjoyed': True, 'you': False, 'it': True, 'like': False, 'I': True, 'hate': False}, 'pos')\n",
      "({'enjoyed': False, 'you': False, 'it': True, 'like': False, 'I': True, 'hate': True}, 'neg')\n"
     ]
    }
   ],
   "source": [
    "# 2. 각 문장을 단어가 있는지 없는지 여부 표현\n",
    "# 단어의 등장 순서 무시 -> 빈도만을 이용!\n",
    "\n",
    "train_features = []\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    words = word_tokenize(sent)\n",
    "    tmp = dict()\n",
    "    for set_word in all_words:\n",
    "        if set_word in words:\n",
    "            tmp[set_word] = True\n",
    "        else:\n",
    "            tmp[set_word] = False\n",
    "    sent_tup = (tmp, label)\n",
    "    train_features.append(sent_tup)\n",
    "\n",
    "# [('I like you', 'pos'), ('I hate you', 'neg'), ('I enjoyed it', 'pos'), ('I hate it', 'neg')]\n",
    "print(train_features[0])\n",
    "print(train_features[1])\n",
    "print(train_features[2])\n",
    "print(train_features[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 코드 더 간단하게 나타내기\n",
    "\n",
    "train_features = []\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    words = word_tokenize(sent)\n",
    "    tmp = dict()\n",
    "    for set_word in all_words:\n",
    "        tmp[set_word] = (set_word in words) # set_word가 words에 있는지 여부 판별\n",
    "    sent_tup = (tmp, label)\n",
    "    train_features.append(sent_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 코드 더더 간단하게 나타내기\n",
    "\n",
    "train_features = []\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    words = word_tokenize(sent)\n",
    "    tmp = {set_word: (set_word in words) for set_word in all_words} # for문 다음에 오는 명령문을 1줄로 작성\n",
    "    sent_tup = (tmp, label)\n",
    "    train_features.append(sent_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 enjoyed = False             neg : pos    =      1.7 : 1.0\n",
      "                    like = False             neg : pos    =      1.7 : 1.0\n",
      "                       I = True              neg : pos    =      1.0 : 1.0\n",
      "                      it = False             neg : pos    =      1.0 : 1.0\n",
      "                      it = True              neg : pos    =      1.0 : 1.0\n",
      "                     you = False             neg : pos    =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 3) 단어별 확률 계산\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**결과 해석** : 'enjoyed' 라는 단어가 문장에 없으면 (False), neg:pos = 1.7 : 1.0\n",
    "\n",
    "+ 확률 구할 때 0.5 씩 더해주기 때문에 비율이 저리 나옵니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'enjoyed': False, 'you': False, 'it': True, 'like': True, 'I': True, 'hate': False}\n"
     ]
    }
   ],
   "source": [
    "test_sent = 'I like it'\n",
    "\n",
    "words = word_tokenize(test_sent)\n",
    "test_feature = {set_word: (set_word in words) for set_word in all_words}\n",
    "\n",
    "print(test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장에 사용된 단어 중 한 개의 단어라도 학습 데이터에 없었다면?\n",
    "    - 전체 확률이 0 이 되어버림! \n",
    "    - **>> 라플라스 스무딩을 이용!**\n",
    "        - 분모, 분자에 일정한 값 (ex 0.5) 를 더해서 분자가 0이 되는 것을 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 데이터 분류하기\n",
    "\n",
    "- 영어 버전과 동일하게 작성하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('사과가 좋아', 'pos'), ('밤에 먹는 사과는 비추야', 'neg'), ('사과가 잘 익었어 맛있겠다', 'pos')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 입력해주기\n",
    "\n",
    "train = [('사과가 좋아', 'pos'),\n",
    "        ('밤에 먹는 사과는 비추야', 'neg'),\n",
    "        ('사과가 잘 익었어 맛있겠다', 'pos')]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'먹는', '비추야', '좋아', '잘', '사과는', '익었어', '맛있겠다', '사과가', '밤에'}\n"
     ]
    }
   ],
   "source": [
    "# 1. train에 있는 문장들의 단어 set 만들기\n",
    "# set()을 이용하여 집합으로 만들기\n",
    "\n",
    "all_words = set()\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    words = word_tokenize(sent)\n",
    "    for word in words:\n",
    "        all_words.add(word)\n",
    "\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'먹는': False, '비추야': False, '좋아': True, '잘': False, '사과는': False, '익었어': False, '맛있겠다': False, '사과가': True, '밤에': False}, 'pos')\n",
      "({'먹는': True, '비추야': True, '좋아': False, '잘': False, '사과는': True, '익었어': False, '맛있겠다': False, '사과가': False, '밤에': True}, 'neg')\n",
      "({'먹는': False, '비추야': False, '좋아': False, '잘': True, '사과는': False, '익었어': True, '맛있겠다': True, '사과가': True, '밤에': False}, 'pos')\n"
     ]
    }
   ],
   "source": [
    "# 2. 각 문장을 단어가 있는지 없는지 여부로 표현\n",
    "\n",
    "train_features = []\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    words = word_tokenize(sent)\n",
    "    tmp = {set_word: (set_word in words) for set_word in all_words}\n",
    "    sent_tup = (tmp, label)\n",
    "    train_features.append(sent_tup)\n",
    "\n",
    "for i in range(len(train_features)):\n",
    "    print(train_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    맛있겠다 = False             neg : pos    =      1.5 : 1.0\n",
      "                     익었어 = False             neg : pos    =      1.5 : 1.0\n",
      "                       잘 = False             neg : pos    =      1.5 : 1.0\n",
      "                      좋아 = False             neg : pos    =      1.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 3. 단어별 확률 계산\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'먹는': False, '비추야': False, '좋아': False, '잘': False, '사과는': True, '익었어': False, '맛있겠다': False, '사과가': False, '밤에': False}\n"
     ]
    }
   ],
   "source": [
    "test_sent = '사과는 맛있어'\n",
    "\n",
    "words = word_tokenize(test_sent)\n",
    "test_feature = {set_word: (set_word in words) for set_word in all_words}\n",
    "\n",
    "print(test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습하기(1) : 형태소 분석기 적용\n",
    "\n",
    "### 문장을 형태소 단위로 분리해주는 함수 작성\n",
    "\n",
    "\n",
    "* 처음 문장(raw_sent)\n",
    "     * '사과가 좋아'\n",
    "* 형태소 분석 결과 문장(sent)\n",
    "    * [('사과', 'Noun'), ('가', 'Josa'), ('좋다', 'Adjective')]\n",
    "* 리턴 문장(' 'join(pos_sent))\n",
    "    * '사과/Noun 가/Josa 좋다/Adjective'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tokenize(raw_sent):\n",
    "    pos_sent = []\n",
    "    \n",
    "    # raw_sent: 사과가 좋아\n",
    "    sent = okt.pos(raw_sent, norm=True, stem=True)    \n",
    "    # sent: [('사과', 'Noun'), ('가', 'Josa'), ('좋다', 'Adjective')]\n",
    "    \n",
    "    for tup in sent:        \n",
    "        word, tag = tup[0], tup[1]       # tup: ('사과', 'Noun')\n",
    "        word_tag = word + '/' + tag      # word_tag: '사과/Noun'\n",
    "        pos_sent.append(word_tag)\n",
    "    \n",
    "    return ' '.join(pos_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'먹다/Verb', '맛있다/Adjective', '에/Josa', '익다/Verb', '사과/Noun', '밤/Noun', '좋다/Adjective', '가/Josa', '자다/Verb', '비추다/Verb', '는/Josa'}\n"
     ]
    }
   ],
   "source": [
    "# train에 있는 문장들의 단어 set 만들기\n",
    "# set()을 이용하여 집합으로 만들기\n",
    "\n",
    "all_words = set()\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]    \n",
    "    sent = pos_tokenize(sent)    # pos_tokenize 함수 추가\n",
    "    words = word_tokenize(sent)\n",
    "    for word in words:\n",
    "        all_words.add(word)\n",
    "\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'먹다/Verb': False, '맛있다/Adjective': False, '에/Josa': False, '익다/Verb': False, '사과/Noun': True, '밤/Noun': False, '좋다/Adjective': True, '가/Josa': True, '자다/Verb': False, '비추다/Verb': False, '는/Josa': False}, 'pos')\n",
      "({'먹다/Verb': True, '맛있다/Adjective': False, '에/Josa': True, '익다/Verb': False, '사과/Noun': True, '밤/Noun': True, '좋다/Adjective': False, '가/Josa': False, '자다/Verb': False, '비추다/Verb': True, '는/Josa': True}, 'neg')\n",
      "({'먹다/Verb': False, '맛있다/Adjective': True, '에/Josa': False, '익다/Verb': True, '사과/Noun': True, '밤/Noun': False, '좋다/Adjective': False, '가/Josa': True, '자다/Verb': True, '비추다/Verb': False, '는/Josa': False}, 'pos')\n"
     ]
    }
   ],
   "source": [
    "# 학습하기는 같음\n",
    "\n",
    "train_features = []\n",
    "\n",
    "for tup in train:\n",
    "    sent, label = tup[0], tup[1]\n",
    "    sent = pos_tokenize(sent)    # pos_tokenize 함수 추가\n",
    "    words = word_tokenize(sent)\n",
    "    tmp = {set_word: (set_word in words) for set_word in all_words}\n",
    "    sent_tup = (tmp, label)\n",
    "    train_features.append(sent_tup)\n",
    "\n",
    "for i in range(len(train_features)):\n",
    "    print(train_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           맛있다/Adjective = False             neg : pos    =      1.5 : 1.0\n",
      "                 익다/Verb = False             neg : pos    =      1.5 : 1.0\n",
      "                 자다/Verb = False             neg : pos    =      1.5 : 1.0\n",
      "            좋다/Adjective = False             neg : pos    =      1.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'먹다/Verb': False, '맛있다/Adjective': True, '에/Josa': False, '익다/Verb': False, '사과/Noun': True, '밤/Noun': False, '좋다/Adjective': False, '가/Josa': False, '자다/Verb': False, '비추다/Verb': False, '는/Josa': True}\n"
     ]
    }
   ],
   "source": [
    "test_sent = '사과는 맛있어'\n",
    "\n",
    "test_sent = pos_tokenize(test_sent)   # pos_tokenize 함수 추가\n",
    "words = word_tokenize(test_sent)\n",
    "test_feature = {set_word: (set_word in words) for set_word in all_words}\n",
    "\n",
    "print(test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영화 리뷰 데이터\n",
    "\n",
    "개발자 도구를 보면 페이지를 넘어갈 때 마다\n",
    "\n",
    "### 한국어 영화 리뷰 데이터"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
